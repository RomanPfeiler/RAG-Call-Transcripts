{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b165cd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pfeil\\anaconda3\\envs\\local-rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Libraries Loaded.\n"
     ]
    }
   ],
   "source": [
    "# ============================== Setup & imports ==============================\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 1. Vector Library\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 2. Keyword Library\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Hybrid Libraries Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "984f8f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config set. Hybrid Alpha: 0.96\n"
     ]
    }
   ],
   "source": [
    "# ============================== CONFIGURATION ==============================\n",
    "\n",
    "# 1. Hybrid Input Data\n",
    "CUSTOMER_PKL = \"transcript_embeddings_hybrid.pkl\"\n",
    "\n",
    "# 2. Hybrid Balance (The most important setting!)\n",
    "# 0.7 means: 70% importance to Vector (Meaning), 30% to Keywords.\n",
    "# You can tweak this to see what works best for your data.\n",
    "HYBRID_ALPHA = 0.96 \n",
    "\n",
    "# 3. Model Name\n",
    "LOCAL_MODEL_NAME = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "\n",
    "# 4. Evaluation Data\n",
    "EVAL_DIR = \"Evaluation-data\"\n",
    "PROMPT_FILES = [\n",
    "    \"automotive_prompts.jsonl\",\n",
    "    \"home_service_prompts.jsonl\",\n",
    "    \"insurance_prompts.jsonl\",\n",
    "    \"medical_equipment_prompts.jsonl\",\n",
    "]\n",
    "\n",
    "CATEGORIES = [\n",
    "    \"automotive - inbound call\",\n",
    "    \"home service - inbound call\",\n",
    "    \"insurance - outbound call\",\n",
    "    \"medical equipment - outbound call\",\n",
    "]\n",
    "INDEX_TO_CATEGORY = dict(enumerate(CATEGORIES))\n",
    "CATEGORY_TO_INDEX = {c: i for i, c in INDEX_TO_CATEGORY.items()}\n",
    "\n",
    "print(f\"Config set. Hybrid Alpha: {HYBRID_ALPHA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f80f0cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen3 model...\n",
      "Loaded 50 hybrid records.\n"
     ]
    }
   ],
   "source": [
    "# ============================== LOAD RESOURCES ==============================\n",
    "\n",
    "# 1. Load Local Qwen Model\n",
    "print(\"Loading Qwen3 model...\")\n",
    "model = SentenceTransformer(LOCAL_MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# 2. Load Data\n",
    "if not os.path.exists(CUSTOMER_PKL):\n",
    "    raise FileNotFoundError(f\"File '{CUSTOMER_PKL}' not found.\")\n",
    "\n",
    "with open(CUSTOMER_PKL, \"rb\") as f:\n",
    "    customer_records: List[Dict[str, Any]] = pickle.load(f)\n",
    "\n",
    "# Validation\n",
    "rec0 = customer_records[0]\n",
    "assert \"dense_embeddings\" in rec0, \"Missing vector embeddings\"\n",
    "assert \"bm25_tokens\" in rec0, \"Missing BM25 tokens\"\n",
    "\n",
    "print(f\"Loaded {len(customer_records)} hybrid records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e5fb66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== MATH & TEXT UTILS ==============================\n",
    "\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Must match the logic used in data generation.\"\"\"\n",
    "    if not isinstance(text, str): return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    return [t for t in text.split(\" \") if t.strip()]\n",
    "\n",
    "def l2_normalize(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Standard vector normalization for Cosine Similarity.\"\"\"\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    if x.ndim == 1:\n",
    "        n = np.linalg.norm(x) + 1e-12\n",
    "        return x / n\n",
    "    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12\n",
    "    return x / n\n",
    "\n",
    "def min_max_normalize(scores: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Scales an array of scores to range [0, 1].\n",
    "    Used to make BM25 scores compatible with Cosine scores.\n",
    "    \"\"\"\n",
    "    s = np.array(scores)\n",
    "    min_val = np.min(s)\n",
    "    max_val = np.max(s)\n",
    "    \n",
    "    # If all scores are the same (e.g. all 0), return zeros\n",
    "    if max_val - min_val == 0:\n",
    "        return np.zeros_like(s)\n",
    "    \n",
    "    return (s - min_val) / (max_val - min_val)\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"For metrics calculation.\"\"\"\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    x = x - np.max(x) \n",
    "    e = np.exp(x)\n",
    "    return e / (np.sum(e) + 1e-12)\n",
    "\n",
    "def loss_metrics(scores: np.ndarray, correct_idx: int) -> dict:\n",
    "    \"\"\"Standard metrics.\"\"\"\n",
    "    probs = softmax(scores)\n",
    "    cross_entropy = -np.log(probs[correct_idx] + 1e-12)\n",
    "    correct_score = float(scores[correct_idx])\n",
    "    best_other_score = float(np.max(np.delete(scores, correct_idx)))\n",
    "    signed_margin = correct_score - best_other_score \n",
    "    order = np.argsort(-scores)\n",
    "    rank_of_correct = int(np.where(order == correct_idx)[0][0]) + 1 \n",
    "    return {\n",
    "        \"cross_entropy\": cross_entropy,\n",
    "        \"signed_margin\": signed_margin,\n",
    "        \"correct_score\": correct_score,\n",
    "        \"best_other_score\": best_other_score,\n",
    "        \"rank_of_correct\": rank_of_correct,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 80 prompts with Qwen...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:27<00:00,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing prompts for BM25...\n",
      "Prompts ready for Hybrid evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================== PREPARE PROMPTS ==============================\n",
    "\n",
    "# Load prompts\n",
    "def load_jsonl(path):\n",
    "    items = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip(): items.append(json.loads(line))\n",
    "    return items\n",
    "\n",
    "all_prompts = []\n",
    "for fname in PROMPT_FILES:\n",
    "    p = os.path.join(EVAL_DIR, fname)\n",
    "    if os.path.exists(p): all_prompts.extend(load_jsonl(p))\n",
    "\n",
    "# 1. Generate Prompt Vectors (Dense)\n",
    "print(f\"Embedding {len(all_prompts)} prompts with Qwen...\")\n",
    "prompt_texts = [p[\"prompt\"] for p in all_prompts]\n",
    "raw_embeddings = model.encode(prompt_texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "prompt_vectors = l2_normalize(raw_embeddings)\n",
    "\n",
    "# 2. Generate Prompt Tokens (Sparse)\n",
    "print(\"Tokenizing prompts for BM25...\")\n",
    "prompt_tokens_list = [simple_tokenize(t) for t in prompt_texts]\n",
    "\n",
    "# Metadata\n",
    "prompt_ids   = [p[\"id\"] for p in all_prompts]\n",
    "prompt_cats  = [p[\"category\"] for p in all_prompts]\n",
    "prompt_diffs = [p[\"difficulty\"] for p in all_prompts]\n",
    "\n",
    "print(\"Prompts ready for Hybrid evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8edce9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation with Alpha=0.96...\n",
      "Evaluated 10/50 customers...\n",
      "Evaluated 20/50 customers...\n",
      "Evaluated 30/50 customers...\n",
      "Evaluated 40/50 customers...\n",
      "Evaluated 50/50 customers...\n",
      "Total evaluations: 4000\n"
     ]
    }
   ],
   "source": [
    "# ============================== HYBRID EVALUATION LOOP ==============================\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"Starting evaluation with Alpha={HYBRID_ALPHA}...\")\n",
    "\n",
    "for c_idx, rec in enumerate(customer_records, start=1):\n",
    "    \n",
    "    # --- A. Setup Customer Data ---\n",
    "    # 1. Dense: Get 4 vectors\n",
    "    doc_vecs = np.array(rec[\"dense_embeddings\"], dtype=np.float32)\n",
    "    doc_vecs = l2_normalize(doc_vecs) # Ensure normalized\n",
    "    \n",
    "    # 2. Sparse: Setup BM25 index for these 4 docs\n",
    "    doc_tokens = rec[\"bm25_tokens\"]\n",
    "    bm25_engine = BM25Okapi(doc_tokens)\n",
    "\n",
    "    # --- B. Evaluate all Prompts ---\n",
    "    for p_idx in range(len(all_prompts)):\n",
    "        \n",
    "        # 1. Dense Scoring (Cosine)\n",
    "        # Shape: (4,) array of scores between -1 and 1 (usually 0.4 to 0.9)\n",
    "        q_vec = prompt_vectors[p_idx]\n",
    "        dense_scores_raw = doc_vecs @ q_vec\n",
    "        \n",
    "        # 2. Sparse Scoring (BM25)\n",
    "        # Shape: (4,) array of scores (e.g., [0.0, 12.5, 4.2, 0.0])\n",
    "        q_tokens = prompt_tokens_list[p_idx]\n",
    "        sparse_scores_raw = np.array(bm25_engine.get_scores(q_tokens))\n",
    "        \n",
    "        # 3. Normalization\n",
    "        # We assume Dense is already roughly 0-1 (cosine). \n",
    "        # We MUST normalize Sparse to 0-1 to combine them fairly.\n",
    "        sparse_scores_norm = min_max_normalize(sparse_scores_raw)\n",
    "        \n",
    "        # 4. Hybrid Fusion (Weighted Sum)\n",
    "        final_scores = (HYBRID_ALPHA * dense_scores_raw) + \\\n",
    "                       ((1 - HYBRID_ALPHA) * sparse_scores_norm)\n",
    "        \n",
    "        # --- C. Metrics & Storage ---\n",
    "        top_idx = int(np.argmax(final_scores))\n",
    "        \n",
    "        pcat = prompt_cats[p_idx]\n",
    "        correct_idx = CATEGORY_TO_INDEX[pcat]\n",
    "        \n",
    "        metrics = loss_metrics(final_scores, correct_idx)\n",
    "        pred_category = INDEX_TO_CATEGORY[top_idx]\n",
    "        is_correct = int(pred_category == pcat)\n",
    "\n",
    "        row = {\n",
    "            \"customer_row\": c_idx,\n",
    "            \"prompt_id\": prompt_ids[p_idx],\n",
    "            \"prompt_category\": pcat,\n",
    "            \"prompt_difficulty\": prompt_diffs[p_idx],\n",
    "            \"predicted_category\": pred_category,\n",
    "            \"is_correct\": is_correct,\n",
    "            \"top_score\": float(final_scores[top_idx]),\n",
    "            # Debug info: Let's see how the models voted!\n",
    "            \"raw_dense_winner\": int(np.argmax(dense_scores_raw)),\n",
    "            \"raw_sparse_winner\": int(np.argmax(sparse_scores_raw)),\n",
    "            **metrics,\n",
    "        }\n",
    "        results.append(row)\n",
    "\n",
    "    if c_idx % 10 == 0:\n",
    "        print(f\"Evaluated {c_idx}/{len(customer_records)} customers...\")\n",
    "\n",
    "df_hybrid = pd.DataFrame(results)\n",
    "print(f\"Total evaluations: {len(df_hybrid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1780962b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hybrid Accuracy (Alpha=0.96) ===\n",
      "Overall: 51.15%\n",
      "\n",
      "=== Accuracy by Category × Difficulty ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_category</th>\n",
       "      <th>prompt_difficulty</th>\n",
       "      <th>accuracy_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>automotive - inbound call</td>\n",
       "      <td>easy</td>\n",
       "      <td>60.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>automotive - inbound call</td>\n",
       "      <td>hard</td>\n",
       "      <td>50.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>home service - inbound call</td>\n",
       "      <td>easy</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>home service - inbound call</td>\n",
       "      <td>hard</td>\n",
       "      <td>60.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>insurance - outbound call</td>\n",
       "      <td>easy</td>\n",
       "      <td>46.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>insurance - outbound call</td>\n",
       "      <td>hard</td>\n",
       "      <td>37.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>medical equipment - outbound call</td>\n",
       "      <td>easy</td>\n",
       "      <td>45.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>medical equipment - outbound call</td>\n",
       "      <td>hard</td>\n",
       "      <td>39.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     prompt_category prompt_difficulty  accuracy_pct\n",
       "0          automotive - inbound call              easy          60.4\n",
       "1          automotive - inbound call              hard          50.6\n",
       "2        home service - inbound call              easy          69.0\n",
       "3        home service - inbound call              hard          60.2\n",
       "4          insurance - outbound call              easy          46.4\n",
       "5          insurance - outbound call              hard          37.6\n",
       "6  medical equipment - outbound call              easy          45.6\n",
       "7  medical equipment - outbound call              hard          39.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Accuracy by Category ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_category</th>\n",
       "      <th>accuracy_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>automotive - inbound call</td>\n",
       "      <td>55.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>home service - inbound call</td>\n",
       "      <td>64.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>insurance - outbound call</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>medical equipment - outbound call</td>\n",
       "      <td>42.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     prompt_category  accuracy_pct\n",
       "0          automotive - inbound call          55.5\n",
       "1        home service - inbound call          64.6\n",
       "2          insurance - outbound call          42.0\n",
       "3  medical equipment - outbound call          42.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Accuracy by Difficulty ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_difficulty</th>\n",
       "      <th>accuracy_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>easy</td>\n",
       "      <td>55.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hard</td>\n",
       "      <td>46.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_difficulty  accuracy_pct\n",
       "0              easy         55.35\n",
       "1              hard         46.95"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================  METRICS & TABLES ==============================\n",
    "\n",
    "# Use the dataframe created in the loop above\n",
    "df = df_hybrid \n",
    "\n",
    "if len(df) > 0:\n",
    "    def pct(x: pd.Series) -> float:\n",
    "        return 100.0 * x.mean()\n",
    "\n",
    "    # 1. Accuracy by Category × Difficulty\n",
    "    # This shows you exactly which specific areas are failing (e.g., Hard Insurance calls)\n",
    "    pivot_cat_diff = (\n",
    "        df\n",
    "        .groupby([\"prompt_category\", \"prompt_difficulty\"])[\"is_correct\"]\n",
    "        .apply(pct)\n",
    "        .rename(\"accuracy_pct\")\n",
    "        .reset_index()\n",
    "        .sort_values([\"prompt_category\", \"prompt_difficulty\"])\n",
    "    )\n",
    "\n",
    "    # 2. Accuracy by Category (Summary)\n",
    "    pivot_cat = (\n",
    "        df\n",
    "        .groupby(\"prompt_category\")[\"is_correct\"]\n",
    "        .apply(pct)\n",
    "        .rename(\"accuracy_pct\")\n",
    "        .reset_index()\n",
    "        .sort_values(\"prompt_category\")\n",
    "    )\n",
    "\n",
    "    # 3. Accuracy by Difficulty (Summary)\n",
    "    pivot_diff = (\n",
    "        df\n",
    "        .groupby(\"prompt_difficulty\")[\"is_correct\"]\n",
    "        .apply(pct)\n",
    "        .rename(\"accuracy_pct\")\n",
    "        .reset_index()\n",
    "        .sort_values(\"prompt_difficulty\")\n",
    "    )\n",
    "\n",
    "    # 4. Overall Accuracy\n",
    "    overall_accuracy_pct = pct(df[\"is_correct\"])\n",
    "\n",
    "    print(f\"=== Hybrid Accuracy (Alpha={HYBRID_ALPHA}) ===\")\n",
    "    print(f\"Overall: {overall_accuracy_pct:.2f}%\")\n",
    "\n",
    "    print(\"\\n=== Accuracy by Category × Difficulty ===\")\n",
    "    display(pivot_cat_diff)\n",
    "\n",
    "    print(\"\\n=== Accuracy by Category ===\")\n",
    "    display(pivot_cat)\n",
    "\n",
    "    print(\"\\n=== Accuracy by Difficulty ===\")\n",
    "    display(pivot_diff)\n",
    "\n",
    "    # ============================== PART 8: LOSS & DIAGNOSTICS ==============================\n",
    "    \n",
    "    # Calculate 'deficit':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16c32c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
