{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97fe9b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Imports\n",
    "# ==============================================================================\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import re # We use Regex for cleaner word splitting\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# NEW: The library for BM25 ranking\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# ==============================================================================\n",
    "# Config\n",
    "# ==============================================================================\n",
    "INPUT_PKL  = \"repackaged_transcript_data.pkl\"\n",
    "\n",
    "# We change the name to indicate these are NOT vector embeddings anymore\n",
    "OUTPUT_PKL = \"transcript_data_bm25_ready.pkl\"\n",
    "\n",
    "CUSTOMERS_TO_PROCESS = 50\n",
    "\n",
    "# We don't need retries/sleep because this runs locally on your CPU instantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aebef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Helpers\n",
    "# ==============================================================================\n",
    "\n",
    "def atomic_pickle(obj, path: str):\n",
    "    \"\"\"Saves data to a temporary file first to prevent corruption.\"\"\"\n",
    "    tmp = path + \".tmp\"\n",
    "    with open(tmp, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def load_input() -> List[List[str]]:\n",
    "    with open(INPUT_PKL, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def load_existing() -> List[Dict[str, Any]]:\n",
    "    if not os.path.exists(OUTPUT_PKL):\n",
    "        return []\n",
    "    with open(OUTPUT_PKL, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits text into words (tokens) for BM25.\n",
    "    1. Lowercase everything (so 'Help' and 'help' are the same).\n",
    "    2. Remove punctuation.\n",
    "    3. Split by whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # Replace non-alphanumeric chars with spaces\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    # Split by spaces and remove empty strings\n",
    "    tokens = [t for t in text.split(\" \") if t.strip()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51849d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 customer records.\n",
      "Processing 50 customers for BM25 (Tokenization)...\n",
      "\n",
      "Done. Saved 50 customers to 'transcript_data_bm25_ready.pkl'.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Main Execution\n",
    "# ==============================================================================\n",
    "def main():\n",
    "    # 1. Load Data\n",
    "    try:\n",
    "        customer_data = load_input()\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Could not find '{INPUT_PKL}'.\")\n",
    "    print(f\"Loaded {len(customer_data)} customer records.\")\n",
    "\n",
    "    # 2. Resume Logic\n",
    "    processed = load_existing()\n",
    "    done_ids = {rec[\"customer_id\"] for rec in processed}\n",
    "    total = min(CUSTOMERS_TO_PROCESS, len(customer_data))\n",
    "    \n",
    "    print(f\"Processing {total} customers for BM25 (Tokenization)...\")\n",
    "\n",
    "    # 3. Loop through customers\n",
    "    for i, transcripts in enumerate(customer_data[:total]):\n",
    "        if i in done_ids:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Tokenize all 4 transcripts for this customer\n",
    "            # We store the TOKENS, not vectors.\n",
    "            tokenized_transcripts = []\n",
    "            for t in transcripts:\n",
    "                tokens = simple_tokenize(t)\n",
    "                tokenized_transcripts.append(tokens)\n",
    "\n",
    "            # Create the record\n",
    "            # We keep the raw 'transcripts' for reading later\n",
    "            # We add 'bm25_tokens' for the search engine to use later\n",
    "            rec = {\n",
    "                \"customer_id\": i, \n",
    "                \"transcripts\": transcripts, \n",
    "                \"bm25_tokens\": tokenized_transcripts\n",
    "            }\n",
    "            processed.append(rec)\n",
    "\n",
    "            # Save\n",
    "            atomic_pickle(processed, OUTPUT_PKL)\n",
    "            \n",
    "            # Print progress every 10 customers (it's very fast now)\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(f\"Processed customer {i+1}/{total}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on customer {i+1}: {e}\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nDone. Saved {len(processed)} customers to '{OUTPUT_PKL}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da4c888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 records for checking.\n",
      "\n",
      "Keys in record: dict_keys(['customer_id', 'transcripts', 'bm25_tokens'])\n",
      "Number of token lists: 4 (Should be 4)\n",
      "First transcript token count: 397\n",
      "First 5 tokens of transcript 0: ['thank', 'you', 'for', 'calling', 'organization']\n",
      "\n",
      "--- Simulating BM25 Search for Customer 0 ---\n",
      "Test Query: 'billing issue with my credit card'\n",
      "Query Tokens: ['billing', 'issue', 'with', 'my', 'credit', 'card']\n",
      "\n",
      "BM25 Scores for the 4 transcripts: [0.22333747 0.13214417 0.19896963 2.67638632]\n",
      "Winner is Transcript Index: 3\n",
      "Winner Score: 2.6764\n",
      "Winner Text Snippet: Hello? Hello? Hello? Hello, Hi, this is [PERSON_NAME]. [PERSON_NAME], I'm sorry to bother you right ...\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Check file contents / Validation\n",
    "# ==============================================================================\n",
    "import pickle\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Load the file we just made\n",
    "INPUT_PKL_CHECK = \"transcript_data_bm25_ready.pkl\"\n",
    "\n",
    "def load_pickle(path: str) -> List[Dict[str, Any]]:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Re-define tokenizer for the test query\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    import re\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    return [t for t in text.split(\" \") if t.strip()]\n",
    "\n",
    "def main_check():\n",
    "    if not os.path.exists(INPUT_PKL_CHECK):\n",
    "        print(\"File not found.\")\n",
    "        return\n",
    "\n",
    "    data = load_pickle(INPUT_PKL_CHECK)\n",
    "    print(f\"Loaded {len(data)} records for checking.\")\n",
    "\n",
    "    # 1. Structure Check\n",
    "    first_rec = data[0]\n",
    "    print(\"\\nKeys in record:\", first_rec.keys())\n",
    "    \n",
    "    # Check if we have 4 lists of tokens\n",
    "    tokens = first_rec[\"bm25_tokens\"]\n",
    "    print(f\"Number of token lists: {len(tokens)} (Should be 4)\")\n",
    "    print(f\"First transcript token count: {len(tokens[0])}\")\n",
    "    print(f\"First 5 tokens of transcript 0: {tokens[0][:5]}\")\n",
    "\n",
    "    # 2. REAL BM25 TEST\n",
    "    # We will simulate how the RAG will work later.\n",
    "    print(\"\\n--- Simulating BM25 Search for Customer 0 ---\")\n",
    "    \n",
    "    # A. Build the index for this specific customer\n",
    "    bm25 = BM25Okapi(tokens)\n",
    "    \n",
    "    # B. Define a test query\n",
    "    query_str = \"billing issue with my credit card\"\n",
    "    query_tokens = simple_tokenize(query_str)\n",
    "    print(f\"Test Query: '{query_str}'\")\n",
    "    print(f\"Query Tokens: {query_tokens}\")\n",
    "    \n",
    "    # C. Get scores for the 4 transcripts\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    print(f\"\\nBM25 Scores for the 4 transcripts: {scores}\")\n",
    "    \n",
    "    # D. Find the winner\n",
    "    best_idx = np.argmax(scores)\n",
    "    print(f\"Winner is Transcript Index: {best_idx}\")\n",
    "    print(f\"Winner Score: {scores[best_idx]:.4f}\")\n",
    "    \n",
    "    # Print a snippet of the winner\n",
    "    winner_text = first_rec[\"transcripts\"][best_idx]\n",
    "    print(f\"Winner Text Snippet: {winner_text[:100]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c9532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
